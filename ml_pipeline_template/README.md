## ğŸ§  ML + LLM Pipeline Orchestration ğŸš€ 
A full-cycle orchestration project that integrates a Machine Learning model pipeline with a basic Retrieval-Augmented Generation (RAG) LLM system using **Airflow**, **MLflow**, **Pandas**, **FastAPI**, and **Docker**.


## ğŸ“‚ Project Structure
ml_llm_pipeline_template/
â”‚
â”œâ”€â”€ ml_pipeline/                     # Part A: ML pipeline
â”‚   â”œâ”€â”€ dags/                        # Airflow DAGs
|   |   â”œâ”€â”€ ml_pipeline_dag.py                
â”‚   â”œâ”€â”€ scripts/                      # Python scripts for each ML task       
|   |   â”œâ”€â”€ingest_data.py
|   |   â”œâ”€â”€preprocess.py
|   |   |â”€â”€feature_engineering.py
|   |   â”œâ”€â”€train_model.py
|   |   â”œâ”€â”€evaluate_model.py
|   |   â”œâ”€â”€log_to_mlflow.py
â”‚   â”œâ”€â”€ model_service/                # FastAPI app for inference
|   |   â”œâ”€â”€Dockerfile
|   |   |â”€â”€main.py
|   |   â”œâ”€â”€requirements.txt     
â”‚   â”œâ”€â”€ data/                         # Raw, processed, and features
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ rag_pipeline/                    # Part B: RAG pipeline
â”‚   â”œâ”€â”€ app/                          # FastAPI + FAISS app
|   |   â”œâ”€â”€Dockerfile
|   |   |â”€â”€main.py
|   |   â”œâ”€â”€requirements.txt 
|   |   â”œâ”€â”€vector_store.py
â”‚   â”œâ”€â”€ docs/                         # Input PDFs
â”‚   â”œâ”€â”€ embeddings/                   # Vector index and chunks
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ mlflow/                    # MLflow logs and model artifacts
â”‚
â”œâ”€â”€ docker-compose.yml         # Orchestrates all services
â”œâ”€â”€ .env (optional)            # Environment variables
â””â”€â”€ README.md                  # You're here!

## ğŸ§ª Part A: ML Pipeline (Wine Quality Prediction)
### âš™ï¸ Technologies:
- Airflow (orchestration)
- Pandas (feature engineering)
- scikit-learn (model training)
- MLflow (model tracking)
- FastAPI (model serving)
- Docker (containerization)

### ğŸ” Workflow:
1. **Ingest**: Load CSV (winequality-red) 
2. **Preprocess**: Clean and transform the data
3. **Feature Engineering**: Generate new features
4. **Train**: Build a classifier using scikit-learn
5. **Evaluate**: Save metrics
6. **Log**: Register model in MLflow
7. **Serve**: Deploy model via FastAPI
8. **Orchestrate**: All steps run through Airflow

### ğŸ›  Setup Instructions
# 1. Clone this repository or unzip the folder
git clone <your-repo-url>
cd ml_llm_pipeline_template

# 2. Start all services
docker-compose up --build


## ğŸ” Testing Part A Model API
curl -X POST http://localhost:8000/predict \
-H "Content-Type: application/json" \
-d '{"data": [[7.4, 0.7, 0.0, 1.9, 0.076, 11, 34, 0.9978, 3.51, 0.56, 9.4]]}'

Youâ€™ll get:
{"predictions": [5]}


## ğŸ§  Part B: RAG LLM Pipeline (Mini POC)
### âš™ï¸ Technologies:
* FAISS (vector store)
* DistilBERT or similar model (via Hugging Face Transformers)
* PDF Ingestion + Chunking
* FastAPI (query handling)
* Docker

### ğŸ” Workflow:
1. Upload PDFs (`docs/`)
2. Split and embed documents â†’ FAISS index
3. Accept query via REST
4. Retrieve top relevant chunks
5. Use LLM to answer based on context

### ğŸ” Sample RAG API Query
curl -X GET "http://localhost:8001/query?q=What is the project about?"

Response:
{
  "query": "What is the project about?",
  "answer": "This project demonstrates a full-cycle ML pipeline and a simple LLM-based RAG system..."
}


## ğŸ¯ Airflow DAG
View and run DAGs from the Airflow UI:
* DAG ID: `wine_quality_pipeline`
* Weekly schedule (`@weekly`)
* Trigger manually or enable auto-retrain


## ğŸ“¦ MLflow Artifacts
Track experiments at: `http://localhost:5000`
* Registered models
* Metrics
* Parameters
* Artifacts (model.pkl, conda.yaml, etc.)


## ğŸ› ï¸ How to Rebuild and Run the ML Pipeline Project from Scratch

If you are setting up the project fresh using this template, follow the steps below:
### ğŸ“ 1. Extract the Project
unzip ml_pipeline_template.zip
cd ml_pipeline_template

### ğŸ§¼ 2. (Optional) Clean Up Previous Docker Artifacts
docker system prune -af --volumes

### ğŸ“¦ 3. Build All Docker Images
docker-compose build --no-cache

If you face Docker Hub pull limits, run:
docker login

### ğŸš€ 4. Start All Containers
docker-compose up -d

This will spin up the following services:

| Service         | Purpose                      | Port                         |
| --------------- | ---------------------------- | ---------------------------- |
| `airflow`       | Workflow orchestrator        | `http://localhost:8080`      |
| `mlflow`        | ML experiment tracking       | `http://localhost:5000`      |
| `model_service` | FastAPI for model serving    | `http://localhost:8000/docs` |
| `rag_service`   | FastAPI for RAG pipeline     | `http://localhost:8001/docs` |
| `spark`         | Spark job for data ingestion | â€”                            |
| `postgres`      | Airflow backend DB           | â€”                            |

### ğŸ¥ª 5. Re-register the ML Model
Since model artifacts are stored in Docker volumes, you must **retrain and register the model**:
docker-compose exec airflow python /scripts/train_model.py

âœ… This will:
* Train a model
* Register it as `sklearn-model` in MLflow
* Save artifacts in `/mlflow`

### ğŸ” 6. Restart Model Service
docker-compose restart model_service

### ğŸ“¡ 7. Test the Model API
Check if it works:
curl http://localhost:8000/docs

Or run a prediction:
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{"data": [[7.4, 0.7, 0, 1.9, 0.076, 11, 34, 0.9978, 3.51, 0.56, 9.4]]}'

### ğŸ§  8. If Spark/Airflow Fails (Optional Debug)
Let Spark/Airflow start, and verify:
* Python + PySpark are installed
* Volume paths are correctly mounted
* Scripts are reachable (`/scripts/ingest_data.py`, etc.)

### âœ… Done!
You're now running the complete ML pipeline again from scratch. ğŸ‰


